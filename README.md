# benchmark-FP32-FP16-INT8-in-pytorch
Benchmark inference speed of CNNs with various quantization methods in Pytorch!

Hardware:
AWS p2.xlarge with K80 GPU.

## FP32 vs FP16
`inference_FP32_vs_FP16.ipynb`

Benchmarks inference speed with FP32 and FP16 with amp.

